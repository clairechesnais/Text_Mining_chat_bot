{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification de la question en domaine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pré-traitement textuel de la base de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Préparation du fichier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous importons le corpus de la composante conversationnelle Opus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('../sources_opus/OpenSubtitles_fr.txt', 'r', encoding='utf-8') as f:\n",
    "    data_opus = f.readlines() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_lignes = len(data_opus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample, seed\n",
    "seed(1234)   # pour la reproductibilité des résultats\n",
    "index_taked = sample(range(0,nb_lignes), 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lignes_opus = [data_opus[i] for i in index_taked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_opus = pd.DataFrame({'texte' : lignes_opus, 'domaine' : 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous importons la base créée dans le fchier .ipynb `Classification de la question en thématiques métier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_faq = pd.read_pickle('df_classif_theme.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_faq['domaine'] = 1\n",
    "df_faq = df_faq[['texte', 'domaine']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_domaine = pd.concat([df_opus, df_faq])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous sauvegardons le DataFrame `df_domaine`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_domaine.to_pickle('df_classif_domaine.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Nettoyage des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons procéder à un nettoyage du tableau de données `df_domaine`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous définissons un certain nombre de fonctions qui nous permettrons de nettoyer le corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('french')\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "\n",
    "sw = nltk.corpus.stopwords.words('french')\n",
    "sw += ['être', 'avoir']\n",
    "sw.sort()\n",
    "\n",
    "def lemmatise_text(text):\n",
    "    lst_lematised = [token.lemma_ for token in nlp(text)] \n",
    "    return ' '.join(lst_lematised).lower()\n",
    "\n",
    "\n",
    "def stem_text(text):\n",
    "    lst_stemmerised = [stemmer.stem(token) for token in word_tokenize(text)]    \n",
    "    return ' '.join(lst_stemmerised)\n",
    "\n",
    "\n",
    "def substitute_punctuation(text):\n",
    "    return ' '.join(text.replace(\"'\", ' ').translate(str.maketrans('', '', string.punctuation)).split())\n",
    "\n",
    "\n",
    "def substitute_special_char(text):\n",
    "    return text.replace(\"«\", \"\").replace(\"’\", \"\").replace(\"•\", \"\").replace(\"®\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous séparons le tableau `df_domaine` en un jeu d'apprentissage et un jeu de test. Nous conservons 70% des données dans le jeu d'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_domaine = pd.read_pickle('df_classif_domaine.pkl')\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_domaine['texte'], \n",
    "                                                    df_domaine['domaine'],\n",
    "                                                    train_size=0.7,\n",
    "                                                    random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_clean = (X_train.apply(lemmatise_text)\n",
    "                        .apply(stem_text)\n",
    "                        .apply(substitute_punctuation)\n",
    "                        .apply(substitute_special_char)\n",
    "                )\n",
    "\n",
    "X_test_clean = (X_test.apply(lemmatise_text)\n",
    "                      .apply(stem_text)\n",
    "                      .apply(substitute_punctuation)\n",
    "                      .apply(substitute_special_char)\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test de différents modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous importons le classifieur TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "vectoriser_theme = load('vectorizer_classif_theme.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant procéder aux tests de différents classifieurs.\n",
    "\n",
    "Nous entraînerons des modèles de classification appartenant à quelques familles d'algorithmes d'apprentissage automatique classique. L'objectif est de comparer non seulement les performances des différentes méthodes entre elles, mais aussi la performance d'une même méthode sur des représentations différentes du texte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Régression Logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_clean_vectorized_tfidf = vectoriser_theme.transform(X_train_clean)\n",
    "X_test_clean_vectorized_tfidf = vectoriser_theme.transform(X_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8387096774193549"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lr =  LogisticRegression(multi_class = 'multinomial', solver='lbfgs', max_iter=500).fit(X_train_clean_vectorized_tfidf, y_train)\n",
    "predictions_valid = model_lr.predict(X_test_clean_vectorized_tfidf)\n",
    "accuracy_score(y_test, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8387096774193549"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lr =  LogisticRegression(multi_class = 'multinomial', solver='newton-cg', max_iter=100, penalty=\"l2\").fit(X_train_clean_vectorized_tfidf, y_train)\n",
    "predictions_valid = model_lr.predict(X_test_clean_vectorized_tfidf)\n",
    "accuracy_score(y_test, predictions_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9032258064516129"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_svm = svm.SVC(kernel='linear', C=10).fit(X_train_clean_vectorized_tfidf, y_train)\n",
    "predictions_valid = model_svm.predict(X_test_clean_vectorized_tfidf)\n",
    "accuracy_score(y_test, predictions_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Choix et sauvegarde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien que nous n'ayons pas laissé la trace de nos recherches, nous avons cherché, pour les deux modèles précédents, à ajuster au mieux leurs paramètres. Après ces différents tests, nous sauvegardons le modèle associé au classifieur qui nous fournit la meilleure accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_classif_domaine.joblib']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "dump(model_svm, 'model_classif_domaine.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation quantitative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin d'évaluer de manière quantitative le modèle sélectionné, nous avons créé un jeu de questions - domaine. Chacune de ces questions est associé au domaine correspondant (1 correspond au domaine métier, 0 ne correspond pas). Les questions sont un mélange de celles de la FAQ ainsi que de celles qui ont été imaginées par nos soins. L'objectif étant de comparer la prédiction du modèle choisi au domaine attendu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>domaine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Quels sont les jours d’arrivée ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Comment évaluer le confort de mon domaine et d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Quels sont les services et activités compris d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Comment réserver mes activités ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Où trouver le plan du domaine ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  domaine\n",
       "0                   Quels sont les jours d’arrivée ?        1\n",
       "1  Comment évaluer le confort de mon domaine et d...        1\n",
       "2  Quels sont les services et activités compris d...        1\n",
       "3                   Comment réserver mes activités ?        1\n",
       "4                    Où trouver le plan du domaine ?        1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_quanti = pd.read_csv('jeu_test_domaine.csv', \";\")\n",
    "df_test_quanti.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous appliquons le même traitement que précédemment à ce jeu de données `df_test_quanti`. Nous séparons la variable d'intérêt de la variable explicative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_quanti = df_test_quanti.question\n",
    "y_test_quanti = df_test_quanti.domaine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_quanti = (X_test_quanti.apply(lemmatise_text)\n",
    "                              .apply(stem_text)\n",
    "                              .apply(substitute_punctuation)\n",
    "                              .apply(substitute_special_char)\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_quanti_vectorized_tfidf = vectoriser_theme.transform(X_test_quanti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model_svm.predict(X_test_quanti_vectorized_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8294117647058824"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test_quanti, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8667687595712097"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test_quanti, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9625850340136054"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test_quanti, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7883008356545961"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test_quanti, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultats obtenus sont satisfaisants puisque l'introduction de nouvelles questions ne perturbent pas le modèle. Le taux de nouvelles questions bien classées est de près de 83%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
