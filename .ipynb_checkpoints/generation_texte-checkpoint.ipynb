{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ok42PDjlhi4v"
   },
   "source": [
    "# Génération de réponses originales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 103487,
     "status": "ok",
     "timestamp": 1580321211229,
     "user": {
      "displayName": "Enora Kaf",
      "photoUrl": "",
      "userId": "10357119445051287415"
     },
     "user_tz": -60
    },
    "id": "5rfVVfIxiPN-",
    "outputId": "c892b7e7-70f4-4441-85bf-bc21a376363e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/d4/c0cd1057b331bc38b65478302114194bd8e1b9c2bbc06e300935c0e93d90/tensorflow-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n",
      "\u001b[K     |████████████████████████████████| 421.8MB 38kB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.33.6)\n",
      "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.8)\n",
      "Requirement already satisfied, skipping upgrade: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n",
      "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.1)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.17.5)\n",
      "Collecting tensorboard<2.2.0,>=2.1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/23/53ffe290341cd0855d595b0a2e7485932f473798af173bbe3a584b99bb06/tensorboard-2.1.0-py3-none-any.whl (3.8MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8MB 52.3MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.1.8)\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
      "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
      "\u001b[K     |████████████████████████████████| 450kB 63.8MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow) (2.8.0)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/6d/7aae38a9022f982cf8167775c7fc299f203417b698c27080ce09060bba07/google_auth-1.11.0-py2.py3-none-any.whl (76kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 12.7MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.1)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (42.0.2)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (2.21.0)\n",
      "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.2.7)\n",
      "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.0)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.0)\n",
      "\u001b[31mERROR: tensorboard 2.1.0 has requirement grpcio>=1.24.3, but you'll have grpcio 1.15.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.4.0, but you'll have google-auth 1.11.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: google-auth, tensorboard, tensorflow-estimator, tensorflow\n",
      "  Found existing installation: google-auth 1.4.2\n",
      "    Uninstalling google-auth-1.4.2:\n",
      "      Successfully uninstalled google-auth-1.4.2\n",
      "  Found existing installation: tensorboard 1.15.0\n",
      "    Uninstalling tensorboard-1.15.0:\n",
      "      Successfully uninstalled tensorboard-1.15.0\n",
      "  Found existing installation: tensorflow-estimator 1.15.1\n",
      "    Uninstalling tensorflow-estimator-1.15.1:\n",
      "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
      "  Found existing installation: tensorflow 1.15.0\n",
      "    Uninstalling tensorflow-1.15.0:\n",
      "      Successfully uninstalled tensorflow-1.15.0\n",
      "Successfully installed google-auth-1.11.0 tensorboard-2.1.0 tensorflow-2.1.0 tensorflow-estimator-2.1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "google"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DUx54W8Lhi47"
   },
   "source": [
    "Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 33656,
     "status": "ok",
     "timestamp": 1580321284617,
     "user": {
      "displayName": "Enora Kaf",
      "photoUrl": "",
      "userId": "10357119445051287415"
     },
     "user_tz": -60
    },
    "id": "yHD0t4neijt7",
    "outputId": "64087d26-dc94-427c-c2bb-caa27610e394"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1788,
     "status": "ok",
     "timestamp": 1580321289462,
     "user": {
      "displayName": "Enora Kaf",
      "photoUrl": "",
      "userId": "10357119445051287415"
     },
     "user_tz": -60
    },
    "id": "t5G2i1l7ilei",
    "outputId": "221d4620-60c6-45c4-944a-aa2a801de9ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'My Drive'\n"
     ]
    }
   ],
   "source": [
    "!ls \"/content/gdrive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RmCdg6Axhi5H"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/echantillon.txt', header = None, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7QQZUqvshi5d"
   },
   "source": [
    "Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K2dYECJmhi5i"
   },
   "outputs": [],
   "source": [
    "#conda install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qOCTK0i2hi5y"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CPMsl8X4hi6B"
   },
   "outputs": [],
   "source": [
    "text = open('/content/gdrive/My Drive/Colab Notebooks/echantillon.txt', 'rb').read().decode(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 389,
     "status": "ok",
     "timestamp": 1580321305009,
     "user": {
      "displayName": "Enora Kaf",
      "photoUrl": "",
      "userId": "10357119445051287415"
     },
     "user_tz": -60
    },
    "id": "oubjw_X3hi6O",
    "outputId": "584d4292-c72d-4b1b-f101-dd8a155fa72d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 67402508 characters\n"
     ]
    }
   ],
   "source": [
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 363,
     "status": "ok",
     "timestamp": 1580321305957,
     "user": {
      "displayName": "Enora Kaf",
      "photoUrl": "",
      "userId": "10357119445051287415"
     },
     "user_tz": -60
    },
    "id": "xcCmFQophi6d",
    "outputId": "ac7a7f05-4580-4c14-d6f3-aa9960e2a608"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intéressant, ton choix de mots. Putain, elle est la seule chose qui les intéresse. Remets ton cache-nez ! Il faudra lui parler, Mme Kane. Plus fort, blanc-bec. C'était un très beau match. Vous avez raison. Tu dois apprendre comment... Mon scrotum Ce \n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1023,
     "status": "ok",
     "timestamp": 1580321307604,
     "user": {
      "displayName": "Enora Kaf",
      "photoUrl": "",
      "userId": "10357119445051287415"
     },
     "user_tz": -60
    },
    "id": "VbdbuY4vhi6n",
    "outputId": "faf6d2fb-112a-4cc2-ff08-91abe875207e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XSWYMbX0hi60"
   },
   "source": [
    "Vectorisation du texte\n",
    "\n",
    "creation d'un tableau contenant les mappages des caractères aux indices, et d'un tableau contenant les mappage des indices aux caractères."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WsOErEekhi65"
   },
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enregistrement des deux tableaux dans un fichier pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2YGgs6tjhi7D"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('/content/gdrive/My Drive/Colab Notebooks/char2idx.pkl','wb') as f:\n",
    "    pickle.dump(char2idx, f)\n",
    "\n",
    "with open('/content/gdrive/My Drive/Colab Notebooks/idx2char.pkl','wb') as f:\n",
    "    pickle.dump(idx2char, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6149,
     "status": "ok",
     "timestamp": 1580321319830,
     "user": {
      "displayName": "Enora Kaf",
      "photoUrl": "",
      "userId": "10357119445051287415"
     },
     "user_tz": -60
    },
    "id": "JjuPD6iohi7Q",
    "outputId": "818d8966-970f-4c57-9d3b-658cc53351f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([37, 74, 80, ..., 72, 65, 79])"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_as_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sGqSrX5Ehi7f"
   },
   "source": [
    "Nous avons maintenant une représentation numérique de chaque caractère\n",
    "Les caractères sont indexés entre 0 et le nombre de caractères distincts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1924,
     "status": "ok",
     "timestamp": 1580321319832,
     "user": {
      "displayName": "Enora Kaf",
      "photoUrl": "",
      "userId": "10357119445051287415"
     },
     "user_tz": -60
    },
    "id": "qKY1aWNFhi7i",
    "outputId": "c6f11faf-a109-418f-cc7a-75a9531b96ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  ' ' :   0,\n",
      "  '!' :   1,\n",
      "  '\"' :   2,\n",
      "  '#' :   3,\n",
      "  '$' :   4,\n",
      "  '%' :   5,\n",
      "  \"'\" :   6,\n",
      "  '(' :   7,\n",
      "  ')' :   8,\n",
      "  '*' :   9,\n",
      "  '+' :  10,\n",
      "  ',' :  11,\n",
      "  '-' :  12,\n",
      "  '.' :  13,\n",
      "  '/' :  14,\n",
      "  '0' :  15,\n",
      "  '1' :  16,\n",
      "  '2' :  17,\n",
      "  '3' :  18,\n",
      "  '4' :  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1580321320066,
     "user": {
      "displayName": "Enora Kaf",
      "photoUrl": "",
      "userId": "10357119445051287415"
     },
     "user_tz": -60
    },
    "id": "0mE2MGwAhi73",
    "outputId": "7afa4413-42bf-456c-e877-e211b93c6c21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Intéressant, ' ---- characters mapped to int ---- > [ 37  74  80 183  78  65  79  79  61  74  80  11   0]\n"
     ]
    }
   ],
   "source": [
    "# Show how the first 13 characters from the text are mapped to integers\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prédiction du caractère suivant selon le caractère actuel\n",
    "Réseau de neurones récurrents\n",
    "Découpage du texte en séquences\n",
    "Creation d'exemples et de cibles. Par exemple, si la taille de séquence est 6 et que nous avons \"Bonjour\", la séquence d'entrée sera \"Bonjou\" et la séquence cible sera \"onjour\".\n",
    "\n",
    "Pour cela, utilisation de la fonction tf.data.Dataset.from_tensor_slices pour convertir le vecteur de texte en un stream de chaque caractère"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 346,
     "status": "ok",
     "timestamp": 1580321322584,
     "user": {
      "displayName": "Enora Kaf",
      "photoUrl": "",
      "userId": "10357119445051287415"
     },
     "user_tz": -60
    },
    "id": "ri--M8Wvhi8C",
    "outputId": "693ed296-7194-4674-a059-ee378416851b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '!': 1,\n",
       " '\"': 2,\n",
       " '#': 3,\n",
       " '$': 4,\n",
       " '%': 5,\n",
       " \"'\": 6,\n",
       " '(': 7,\n",
       " ')': 8,\n",
       " '*': 9,\n",
       " '+': 10,\n",
       " ',': 11,\n",
       " '-': 12,\n",
       " '.': 13,\n",
       " '/': 14,\n",
       " '0': 15,\n",
       " '1': 16,\n",
       " '2': 17,\n",
       " '3': 18,\n",
       " '4': 19,\n",
       " '5': 20,\n",
       " '6': 21,\n",
       " '7': 22,\n",
       " '8': 23,\n",
       " '9': 24,\n",
       " ':': 25,\n",
       " ';': 26,\n",
       " '=': 27,\n",
       " '@': 28,\n",
       " 'A': 29,\n",
       " 'B': 30,\n",
       " 'C': 31,\n",
       " 'D': 32,\n",
       " 'E': 33,\n",
       " 'F': 34,\n",
       " 'G': 35,\n",
       " 'H': 36,\n",
       " 'I': 37,\n",
       " 'J': 38,\n",
       " 'K': 39,\n",
       " 'L': 40,\n",
       " 'M': 41,\n",
       " 'N': 42,\n",
       " 'O': 43,\n",
       " 'P': 44,\n",
       " 'Q': 45,\n",
       " 'R': 46,\n",
       " 'S': 47,\n",
       " 'T': 48,\n",
       " 'U': 49,\n",
       " 'V': 50,\n",
       " 'W': 51,\n",
       " 'X': 52,\n",
       " 'Y': 53,\n",
       " 'Z': 54,\n",
       " '[': 55,\n",
       " '\\\\': 56,\n",
       " ']': 57,\n",
       " '^': 58,\n",
       " '_': 59,\n",
       " '`': 60,\n",
       " 'a': 61,\n",
       " 'b': 62,\n",
       " 'c': 63,\n",
       " 'd': 64,\n",
       " 'e': 65,\n",
       " 'f': 66,\n",
       " 'g': 67,\n",
       " 'h': 68,\n",
       " 'i': 69,\n",
       " 'j': 70,\n",
       " 'k': 71,\n",
       " 'l': 72,\n",
       " 'm': 73,\n",
       " 'n': 74,\n",
       " 'o': 75,\n",
       " 'p': 76,\n",
       " 'q': 77,\n",
       " 'r': 78,\n",
       " 's': 79,\n",
       " 't': 80,\n",
       " 'u': 81,\n",
       " 'v': 82,\n",
       " 'w': 83,\n",
       " 'x': 84,\n",
       " 'y': 85,\n",
       " 'z': 86,\n",
       " '{': 87,\n",
       " '}': 88,\n",
       " '~': 89,\n",
       " '\\x80': 90,\n",
       " '\\x81': 91,\n",
       " '\\x83': 92,\n",
       " '\\x86': 93,\n",
       " '\\x87': 94,\n",
       " '\\x88': 95,\n",
       " '\\x89': 96,\n",
       " '\\x8a': 97,\n",
       " '\\x8c': 98,\n",
       " '\\x8d': 99,\n",
       " '\\x8e': 100,\n",
       " '\\x8f': 101,\n",
       " '\\x90': 102,\n",
       " '\\x92': 103,\n",
       " '\\x93': 104,\n",
       " '\\x94': 105,\n",
       " '\\x95': 106,\n",
       " '\\x96': 107,\n",
       " '\\x97': 108,\n",
       " '\\x98': 109,\n",
       " '\\x99': 110,\n",
       " '\\x9a': 111,\n",
       " '\\x9c': 112,\n",
       " '\\x9d': 113,\n",
       " '\\x9e': 114,\n",
       " '\\x9f': 115,\n",
       " '\\xa0': 116,\n",
       " '¡': 117,\n",
       " '¢': 118,\n",
       " '£': 119,\n",
       " '¤': 120,\n",
       " '¥': 121,\n",
       " '¦': 122,\n",
       " '§': 123,\n",
       " '¨': 124,\n",
       " '©': 125,\n",
       " 'ª': 126,\n",
       " '¬': 127,\n",
       " '\\xad': 128,\n",
       " '®': 129,\n",
       " '¯': 130,\n",
       " '°': 131,\n",
       " '±': 132,\n",
       " '²': 133,\n",
       " '³': 134,\n",
       " '´': 135,\n",
       " '¶': 136,\n",
       " '·': 137,\n",
       " '¸': 138,\n",
       " '¹': 139,\n",
       " 'º': 140,\n",
       " '¼': 141,\n",
       " '½': 142,\n",
       " '¾': 143,\n",
       " '¿': 144,\n",
       " 'À': 145,\n",
       " 'Á': 146,\n",
       " 'Â': 147,\n",
       " 'Ã': 148,\n",
       " 'Ä': 149,\n",
       " 'Å': 150,\n",
       " 'Æ': 151,\n",
       " 'Ç': 152,\n",
       " 'È': 153,\n",
       " 'É': 154,\n",
       " 'Ê': 155,\n",
       " 'Ë': 156,\n",
       " 'Ì': 157,\n",
       " 'Í': 158,\n",
       " 'Î': 159,\n",
       " 'Ï': 160,\n",
       " 'Ñ': 161,\n",
       " 'Ò': 162,\n",
       " 'Ó': 163,\n",
       " 'Ô': 164,\n",
       " 'Õ': 165,\n",
       " 'Ö': 166,\n",
       " '×': 167,\n",
       " 'Ø': 168,\n",
       " 'Ù': 169,\n",
       " 'Ú': 170,\n",
       " 'Û': 171,\n",
       " 'Ü': 172,\n",
       " 'ß': 173,\n",
       " 'à': 174,\n",
       " 'á': 175,\n",
       " 'â': 176,\n",
       " 'ã': 177,\n",
       " 'ä': 178,\n",
       " 'å': 179,\n",
       " 'æ': 180,\n",
       " 'ç': 181,\n",
       " 'è': 182,\n",
       " 'é': 183,\n",
       " 'ê': 184,\n",
       " 'ë': 185,\n",
       " 'ì': 186,\n",
       " 'í': 187,\n",
       " 'î': 188,\n",
       " 'ï': 189,\n",
       " 'ð': 190,\n",
       " 'ñ': 191,\n",
       " 'ò': 192,\n",
       " 'ó': 193,\n",
       " 'ô': 194,\n",
       " 'õ': 195,\n",
       " 'ö': 196,\n",
       " 'ø': 197,\n",
       " 'ù': 198,\n",
       " 'ú': 199,\n",
       " 'û': 200,\n",
       " 'ü': 201,\n",
       " 'ý': 202,\n",
       " 'þ': 203,\n",
       " 'ÿ': 204,\n",
       " 'Ă': 205,\n",
       " 'ă': 206,\n",
       " 'ą': 207,\n",
       " 'ć': 208,\n",
       " 'č': 209,\n",
       " 'ď': 210,\n",
       " 'ė': 211,\n",
       " 'Ę': 212,\n",
       " 'ę': 213,\n",
       " 'Ě': 214,\n",
       " 'ğ': 215,\n",
       " 'ı': 216,\n",
       " 'Ł': 217,\n",
       " 'ł': 218,\n",
       " 'Œ': 219,\n",
       " 'œ': 220,\n",
       " 'Ŕ': 221,\n",
       " 'ŕ': 222,\n",
       " 'ś': 223,\n",
       " 'Ş': 224,\n",
       " 'ş': 225,\n",
       " 'Š': 226,\n",
       " 'š': 227,\n",
       " 'Ť': 228,\n",
       " 'ť': 229,\n",
       " 'ů': 230,\n",
       " 'ű': 231,\n",
       " 'Ż': 232,\n",
       " 'Ž': 233,\n",
       " 'ž': 234,\n",
       " 'ƒ': 235,\n",
       " 'ǁ': 236,\n",
       " 'ȧ': 237,\n",
       " 'ˀ': 238,\n",
       " 'ˁ': 239,\n",
       " 'ˆ': 240,\n",
       " 'Α': 241,\n",
       " 'Β': 242,\n",
       " 'Ε': 243,\n",
       " 'Ζ': 244,\n",
       " 'Η': 245,\n",
       " 'Ι': 246,\n",
       " 'Μ': 247,\n",
       " 'Ν': 248,\n",
       " 'Ο': 249,\n",
       " 'Τ': 250,\n",
       " 'ΰ': 251,\n",
       " 'β': 252,\n",
       " 'δ': 253,\n",
       " 'η': 254,\n",
       " 'θ': 255,\n",
       " 'ι': 256,\n",
       " 'κ': 257,\n",
       " 'ν': 258,\n",
       " 'ξ': 259,\n",
       " 'ο': 260,\n",
       " 'τ': 261,\n",
       " 'ϋ': 262,\n",
       " 'Ѕ': 263,\n",
       " 'Ј': 264,\n",
       " 'А': 265,\n",
       " 'З': 266,\n",
       " 'С': 267,\n",
       " 'а': 268,\n",
       " 'в': 269,\n",
       " 'е': 270,\n",
       " 'з': 271,\n",
       " 'и': 272,\n",
       " 'й': 273,\n",
       " 'к': 274,\n",
       " 'о': 275,\n",
       " 'р': 276,\n",
       " 'с': 277,\n",
       " 'ф': 278,\n",
       " 'х': 279,\n",
       " 'щ': 280,\n",
       " 'ѕ': 281,\n",
       " 'і': 282,\n",
       " 'Ҫ': 283,\n",
       " 'ҫ': 284,\n",
       " 'ہ': 285,\n",
       " 'ഊ': 286,\n",
       " 'ര': 287,\n",
       " 'ഹ': 288,\n",
       " 'ാ': 289,\n",
       " '൭': 290,\n",
       " 'ᴥ': 291,\n",
       " '\\u200b': 292,\n",
       " '\\u200e': 293,\n",
       " '–': 294,\n",
       " '—': 295,\n",
       " '‚': 296,\n",
       " '‡': 297,\n",
       " '•': 298,\n",
       " '\\u202f': 299,\n",
       " '‰': 300,\n",
       " '⁻': 301,\n",
       " '€': 302,\n",
       " '™': 303,\n",
       " '−': 304,\n",
       " '∰': 305,\n",
       " '∽': 306,\n",
       " '♪': 307,\n",
       " '♫': 308,\n",
       " 'ⴠ': 309,\n",
       " '⼼': 310,\n",
       " '《': 311,\n",
       " '〠': 312,\n",
       " '〬': 313,\n",
       " '〰': 314,\n",
       " '〺': 315,\n",
       " 'て': 316,\n",
       " '㈺': 317,\n",
       " '㐱': 318,\n",
       " '㔬': 319,\n",
       " '㤊': 320,\n",
       " '㤹': 321,\n",
       " '㨰': 322,\n",
       " '㰊': 323,\n",
       " '㵥': 324,\n",
       " '㸭': 325,\n",
       " '扵': 326,\n",
       " '挠': 327,\n",
       " '昣': 328,\n",
       " '晦': 329,\n",
       " '楴': 330,\n",
       " '汯': 331,\n",
       " '汴': 332,\n",
       " '渮': 333,\n",
       " '潦': 334,\n",
       " '牯': 335,\n",
       " '猠': 336,\n",
       " '獥': 337,\n",
       " '獶': 338,\n",
       " '琮': 339,\n",
       " '瑥': 340,\n",
       " '瑮': 341,\n",
       " '眾': 342,\n",
       " '睷': 343,\n",
       " '穩': 344,\n",
       " 'ﬀ': 345,\n",
       " 'ﬁ': 346,\n",
       " 'ﬂ': 347}"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 806,
     "status": "ok",
     "timestamp": 1580321323740,
     "user": {
      "displayName": "Enora Kaf",
      "photoUrl": "",
      "userId": "10357119445051287415"
     },
     "user_tz": -60
    },
    "id": "1PJb_J64hi8N",
    "outputId": "d430c280-bf2b-4c50-be30-2282d914f07f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "n\n",
      "t\n",
      "é\n",
      "r\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "  print(idx2char[i.numpy()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y22Z6CAXhi8V"
   },
   "source": [
    "La méthode batch nous permet de convertir facilement les caractères individuels en séquences de la taille désirée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 487,
     "status": "ok",
     "timestamp": 1580321326448,
     "user": {
      "displayName": "Enora Kaf",
      "photoUrl": "",
      "userId": "10357119445051287415"
     },
     "user_tz": -60
    },
    "id": "Oe1FkvhIhi8Y",
    "outputId": "5dafef51-25f8-4c15-9623-0ede3f577b80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Intéressant, ton choix de mots. Putain, elle est la seule chose qui les intéresse. Remets ton cache-n'\n",
      "\"ez ! Il faudra lui parler, Mme Kane. Plus fort, blanc-bec. C'était un très beau match. Vous avez rais\"\n",
      "\"on. Tu dois apprendre comment... Mon scrotum Ce n'est pas très sur. Alors, vous étiez amis toi et le \"\n",
      "\"gargarisme. Abandonnez. Inutile, me voici. Bon, peut-être que je n'aurais pas dû en parler maintenant\"\n",
      "\". Ce n'était pas mon intention. Non, je ne parle pas ukrainien, mais je parle enveloppe de cash. Je p\"\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "  print(repr(''.join(idx2char[item.numpy()])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x5ya4ZxLhi8j"
   },
   "source": [
    "Nous avons dupliqué et shift chaque séquence pour former le texte d'entrée et cible en utilisant une méthode map pour appliquer une fonction simple à chaque batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qizieHLahi8n"
   },
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 344,
     "status": "ok",
     "timestamp": 1580321331023,
     "user": {
      "displayName": "Enora Kaf",
      "photoUrl": "",
      "userId": "10357119445051287415"
     },
     "user_tz": -60
    },
    "id": "3LaLRH2Yhi8w",
    "outputId": "0dddfdb4-440d-4872-d7a5-3d5279c361f5",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ((100,), (100,)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tkMDw3u-hi86"
   },
   "source": [
    "Premiers exemples :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 520,
     "status": "ok",
     "timestamp": 1580321334016,
     "user": {
      "displayName": "Enora Kaf",
      "photoUrl": "",
      "userId": "10357119445051287415"
     },
     "user_tz": -60
    },
    "id": "CrOrWd8fhi89",
    "outputId": "c1f43cbf-5fdf-4be4-e4ff-14d7b6680f26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'Intéressant, ton choix de mots. Putain, elle est la seule chose qui les intéresse. Remets ton cache-'\n",
      "Target data: 'ntéressant, ton choix de mots. Putain, elle est la seule chose qui les intéresse. Remets ton cache-n'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WvG-3jxThi9F"
   },
   "source": [
    "Chaque index de ces vecteurs est traité comme un pas de temps. Pour l'entrée au pas de temps 0, le modèle reçoit l'index pour \"F\" et essaie de prédire l'index pour \"i\" comme caractère suivant. Au pas de temps suivant, il fait la même chose mais le RNN considère le contexte de l'étape précédente en plus du caractère d'entrée actuel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1063,
     "status": "ok",
     "timestamp": 1580321336187,
     "user": {
      "displayName": "Enora Kaf",
      "photoUrl": "",
      "userId": "10357119445051287415"
     },
     "user_tz": -60
    },
    "id": "s_O_16QZhi9O",
    "outputId": "2470ce20-1594-421d-d983-d8dabed45482"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 37 ('I')\n",
      "  expected output: 74 ('n')\n",
      "Step    1\n",
      "  input: 74 ('n')\n",
      "  expected output: 80 ('t')\n",
      "Step    2\n",
      "  input: 80 ('t')\n",
      "  expected output: 183 ('é')\n",
      "Step    3\n",
      "  input: 183 ('é')\n",
      "  expected output: 78 ('r')\n",
      "Step    4\n",
      "  input: 78 ('r')\n",
      "  expected output: 65 ('e')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6pgEKwDxhi9U"
   },
   "source": [
    "Création des batches d'entraînement\n",
    "\n",
    "Avant d'alimenter le modèle par ces données, nous avons besoin de mélanger les données et de les regrouper en batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 420,
     "status": "ok",
     "timestamp": 1580321339175,
     "user": {
      "displayName": "Enora Kaf",
      "photoUrl": "",
      "userId": "10357119445051287415"
     },
     "user_tz": -60
    },
    "id": "CQFmBbpuhi9X",
    "outputId": "f5f1c699-48ff-42ba-9b0c-eff02e41a643"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ebCKVTbQhi9k"
   },
   "source": [
    "Construction du modèle\n",
    "\n",
    "Utilisation de tf.keras.Sequential pour définir le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M3wTv8fMhi9n"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gtHTJ5XChi9v"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zFurvKSuhi93"
   },
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BJkgflkYhi98"
   },
   "source": [
    "Pour chaque caractère, du modèle recherche l'embedding, fait tourner le GPU  avec un pas de temps avec l'embedding en entrée ; puis applique la couche dense pour générer des logits prédisant la probabilité de log du caractère suivant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entraînement du modèle\n",
    "\n",
    "\n",
    "A présent, nous allons exécuter le modèle pour voir qu'il se comporte comme prévu.\n",
    "\n",
    "Nous vérifions d'abord la forme de la sortie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7959,
     "status": "ok",
     "timestamp": 1580321355148,
     "user": {
      "displayName": "Enora Kaf",
      "photoUrl": "",
      "userId": "10357119445051287415"
     },
     "user_tz": -60
    },
    "id": "4DifoEgahi9_",
    "outputId": "06a2fa24-7796-4614-adde-01a7adef0d49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 348) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 274
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4233,
     "status": "ok",
     "timestamp": 1580321355155,
     "user": {
      "displayName": "Enora Kaf",
      "photoUrl": "",
      "userId": "10357119445051287415"
     },
     "user_tz": -60
    },
    "id": "d-VK2q5Xhi-L",
    "outputId": "b5232d69-864d-4c83-e2c2-4d0c568db981"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           89088     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 348)           356700    \n",
      "=================================================================\n",
      "Total params: 4,384,092\n",
      "Trainable params: 4,384,092\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z9zRjSnvhi-U"
   },
   "source": [
    "Pour obtenir des prédictions réelles du modèle, nous devons échantillonner à partir de la distribution de sortie, pour obtenir des indices de caractères réels. Cette distribution est définie par les logits sur le vocabulaire des caractères.\n",
    "Remarque: Il est important d'échantillonner à partir de cette distribution car prendre l'argmax de la distribution peut facilement bloquer le modèle dans une boucle.\n",
    "\n",
    "Nous essayons dans un premier temps pour le premier exemple du lot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mi9QIN6phi-V"
   },
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qd_3YS_Uhi-a"
   },
   "source": [
    "Cela nous donne, à chaque pas de temps, une prédiction de l'index de caractère suivant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 362,
     "status": "ok",
     "timestamp": 1580321365725,
     "user": {
      "displayName": "Enora Kaf",
      "photoUrl": "",
      "userId": "10357119445051287415"
     },
     "user_tz": -60
    },
    "id": "sztvcJC4hi-d",
    "outputId": "110e9241-81e2-419b-b724-25012363756a",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([312, 192, 261, 116, 198,  20, 345,  35, 264, 136, 293, 274, 131,\n",
       "       150, 131, 197, 205,  51, 322, 148, 143, 255, 100,  13, 231,  56,\n",
       "        18, 338, 312, 196, 206, 149, 192, 174,  73,   7,  62, 247, 102,\n",
       "        43, 171, 294, 134, 142, 111, 138,  48,  86,  23,  46, 242,  67,\n",
       "       311, 338,  22,   3,  70,  61,  33, 107, 198,  68, 259,  66, 201,\n",
       "        96, 250, 310, 188, 209, 277,   2,  42, 335,  86, 193, 271, 153,\n",
       "       296, 320, 329,  85,  46, 157, 235, 128, 336, 258, 342, 118, 125,\n",
       "        97, 262, 334,   5, 161, 294,  80, 179, 274])"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XlYF9t_Khi-k"
   },
   "source": [
    "Nous pouvons les décoder afin de voir le texte prédit par ce modèle non entraîné :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 420,
     "status": "ok",
     "timestamp": 1580321367301,
     "user": {
      "displayName": "Enora Kaf",
      "photoUrl": "",
      "userId": "10357119445051287415"
     },
     "user_tz": -60
    },
    "id": "U-QUxlzkhi-m",
    "outputId": "dfd09aaf-3b70-4166-97e3-3ca3e6afd934"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " \"est là. Comme si j'étais un politicien. une sorte de monarque. {\\\\pos(192,210)}S'il préfère se vautre\"\n",
      "\n",
      "Next Char Predictions: \n",
      " '〠òτ\\xa0ù5ﬀGЈ¶\\u200eк°Å°øĂW㨰Ã¾θ\\x8e.ű\\\\3獶〠öăÄòàm(bΜ\\x90OÛ–³½\\x9a¸Tz8RΒg《獶7#jaE\\x96ùhξfü\\x89Τ⼼îčс\"N牯zóзÈ‚㤊晦yRÌƒ\\xad猠ν眾¢©\\x8aϋ潦%Ñ–tåк'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rPchi9bXhi-s"
   },
   "source": [
    "### Entraînement du modèle\n",
    "\n",
    "À ce stade, le problème peut être traité comme un problème de classification standard. Étant donné l'état précédent du réseau de neurones récurretnts, ainsi que l'entrée du pas de temps, nous pouvons prédire la classe du caractère suivant.\n",
    "Nous attachons alors un optimiseur et une fonction de perte\n",
    "\n",
    "La fonction de perte standard tf.keras.losses.sparse_categorical_crossentropy fonctionne dans ce cas car elle est appliquée sur la dernière dimension des prédictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 383,
     "status": "ok",
     "timestamp": 1580321369645,
     "user": {
      "displayName": "Enora Kaf",
      "photoUrl": "",
      "userId": "10357119445051287415"
     },
     "user_tz": -60
    },
    "id": "BngV2HIbhi-t",
    "outputId": "97a89055-fcfc-4390-e5a1-71c7091a5c4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 348)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       5.8529687\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v1tzx3z7hi-1"
   },
   "source": [
    "Nous avons ensuite configurer la procédure d'entraînement à l'aide de la méthode Model.compile de Tensorflow. Nous avons utilisé optimizers.Adam avec les arguments par défaut et la fonction de perte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yuVoOctHhi-3"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4wIwgrV7hi-7"
   },
   "source": [
    "Configuration des chexkpoints\n",
    "\n",
    "Nous avons utilisé callbacks.ModelCheckpoint pour nous assurer que les checkpoints soient sauvegardés durant l'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h4Q8LFMHhi--"
   },
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = '/content/gdrive/My Drive/Colab Notebooks/training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UwHuhtmXhi_E"
   },
   "source": [
    "### Exécution de l'entraînement\n",
    "\n",
    "Pour garder un temps de traitement raisonable, nous avons choisi d'utiliser  epochs pour entraîner le modèle. (A noter que nous avons lancer le script dans Google Collab afin d'utiliser le GPU et réduire les temps de traitement. Sur un PC standard, un lancement avec 1% de la base OPUS nettoyée et 6 epochs avait pris 7h, contre 30 minutes sur Google Collab avec le GPU. C'est ce qui nous a permis de prendre 15% de la base OPUS nettoyée ainsi que 10 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ddrRfj8ahi_I"
   },
   "outputs": [],
   "source": [
    "EPOCHS=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 385,
     "status": "ok",
     "timestamp": 1580321381449,
     "user": {
      "displayName": "Enora Kaf",
      "photoUrl": "",
      "userId": "10357119445051287415"
     },
     "user_tz": -60
    },
    "id": "aPQX6XhShi_R",
    "outputId": "1aa807e3-a8cf-4c88-ec8b-2e112e61c877"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 403
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6635275,
     "status": "ok",
     "timestamp": 1580328017104,
     "user": {
      "displayName": "Enora Kaf",
      "photoUrl": "",
      "userId": "10357119445051287415"
     },
     "user_tz": -60
    },
    "id": "YZGKU_eYhi_W",
    "outputId": "0ffad74c-68a6-4f72-9724-73bca3836449",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 10427 steps\n",
      "Epoch 1/10\n",
      "10427/10427 [==============================] - 648s 62ms/step - loss: 1.2733\n",
      "Epoch 2/10\n",
      "10427/10427 [==============================] - 666s 64ms/step - loss: 1.1645\n",
      "Epoch 3/10\n",
      "10427/10427 [==============================] - 672s 64ms/step - loss: 1.1654\n",
      "Epoch 4/10\n",
      "10427/10427 [==============================] - 678s 65ms/step - loss: 1.3552\n",
      "Epoch 5/10\n",
      "10427/10427 [==============================] - 686s 66ms/step - loss: 1.8118\n",
      "Epoch 6/10\n",
      "10427/10427 [==============================] - 664s 64ms/step - loss: 1.4334\n",
      "Epoch 7/10\n",
      "10427/10427 [==============================] - 651s 62ms/step - loss: 1.2792\n",
      "Epoch 8/10\n",
      "10427/10427 [==============================] - 660s 63ms/step - loss: 1.2440\n",
      "Epoch 9/10\n",
      "10427/10427 [==============================] - 657s 63ms/step - loss: 1.2332\n",
      "Epoch 10/10\n",
      "10427/10427 [==============================] - 652s 63ms/step - loss: 1.2330\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wDhrdf_uhi_c"
   },
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"/content/gdrive/My Drive/Colab Notebooks/model_generation_text.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"/content/gdrive/My Drive/Colab Notebooks/model_generation_text.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6aKxiPsChi_k"
   },
   "source": [
    "### Génération de texte\n",
    "\n",
    "Restauration du dernier point de contrôle\n",
    "\n",
    "Pour garder cette étape de prédiction simple, nous avons choisi une taille de batch de 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 939,
     "status": "ok",
     "timestamp": 1580328139838,
     "user": {
      "displayName": "Enora Kaf",
      "photoUrl": "",
      "userId": "10357119445051287415"
     },
     "user_tz": -60
    },
    "id": "QLN-k-6whi_m",
    "outputId": "11f78054-feb0-49fd-ad1c-47b4e79caaa4",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/content/gdrive/My Drive/Colab Notebooks/training_checkpoints/ckpt_10'"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_RJFqDnPhi_x"
   },
   "outputs": [],
   "source": [
    "model_generation_text = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model_generation_text.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model_generation_text.build(tf.TensorShape([1, None]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 274
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 651,
     "status": "ok",
     "timestamp": 1580328145736,
     "user": {
      "displayName": "Enora Kaf",
      "photoUrl": "",
      "userId": "10357119445051287415"
     },
     "user_tz": -60
    },
    "id": "k6rgbWTWhi_3",
    "outputId": "351b675b-a7fd-454c-f9bc-6be1c08ab591"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            89088     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 348)            356700    \n",
      "=================================================================\n",
      "Total params: 4,384,092\n",
      "Trainable params: 4,384,092\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_generation_text.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GBbw7CZuhjAB"
   },
   "source": [
    "Boucle de prédiction\n",
    "\n",
    "La fonction suivante permet de générer du texte. Elle prend en entrée le modèle, le nombre de caractères à générer (par défaut 100), ainsi qu'une chaîne de caractères (ce qui correspond à la question de l'utilisateur)\n",
    "  Il démarre en choisissant une chaîne de départ, en initialisant l'état RNN et en définissant le nombre de caractères à générer.\n",
    "\n",
    "Nous obtenons la distribution de prédiction du caractère suivant en utilisant la chaîne de début et l'état RNN.\n",
    "\n",
    "Ensuite, nous utilisons une distribution catégorielle pour calculer l'indice du caractère prédit. Puis nous utilisons ce caractère prédit comme notre prochaine entrée dans le modèle.\n",
    "\n",
    "L'état RNN retourné par le modèle est réinjecté dans le modèle afin qu'il ait désormais plus de contexte, au lieu d'un seul mot. Après avoir prédit le mot suivant, les états RNN modifiés sont à nouveau réinjectés dans le modèle, c'est ainsi qu'il apprend à mesure qu'il obtient plus de contexte à partir des mots prédits précédemment.\n",
    "\n",
    "Pour générer du texte, la sortie du modèle est renvoyée à l'entrée\n",
    "\n",
    "En regardant le texte généré, nous voyons que le modèle sait quand capitaliser, créer des paragraphes et imite un vocabulaire d'écriture similaire à celui étudié dans la base OPUS nettoyée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RW-kRDuuhjAE"
   },
   "outputs": [],
   "source": [
    "def generate_text(model_generation_text, start_string, num_generate = 100):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 1.0\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model_generation_text(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the word returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 869,
     "status": "ok",
     "timestamp": 1580329750752,
     "user": {
      "displayName": "Enora Kaf",
      "photoUrl": "",
      "userId": "10357119445051287415"
     },
     "user_tz": -60
    },
    "id": "vXcFQZ-khjAJ",
    "outputId": "624d74b7-feb0-41e1-f5c6-e60c4e96082e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "okue. Ravissave te voir. C'est Merci. Jusqu'à tes marines. Tu ne gâchis pas de dire: La pigroticulé re\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model_generation_text, start_string=u\"ok\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 681,
     "status": "ok",
     "timestamp": 1580328168930,
     "user": {
      "displayName": "Enora Kaf",
      "photoUrl": "",
      "userId": "10357119445051287415"
     },
     "user_tz": -60
    },
    "id": "3c1SpBiX4eAE",
    "outputId": "a6870172-43c0-4e85-c583-4e0d294087e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67402508"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Amelioration_GAN.ipynb",
   "provenance": [
    {
     "file_id": "1lmN_eQbyxMBtpWzOKYuWXwC8gjgCGBTQ",
     "timestamp": 1580305128848
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
