{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-225-80a28beef703>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('french')\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "\n",
    "sw = nltk.corpus.stopwords.words('french')\n",
    "sw += ['être', 'avoir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_code_theme = {\"Préparer mon séjour\": 1,\n",
    "                  \"Réserver et payer\": 2,\n",
    "                  \"Gérer ma réservation\": 3,\n",
    "                  \"Mon séjour\": 4,\n",
    "                  \"Assurances\": 5}\n",
    "dic_decode_theme = {val: key for key, val in dic_code_theme.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatise_text(text):\n",
    "    lst_lematised = [token.lemma_ for token in nlp(text)] \n",
    "    return ' '.join(lst_lematised).lower()\n",
    "\n",
    "\n",
    "def stem_text(text):\n",
    "    lst_stemmerised = [stemmer.stem(token) for token in word_tokenize(text)]    \n",
    "    return ' '.join(lst_stemmerised)\n",
    "\n",
    "\n",
    "def substitute_punctuation(text):\n",
    "    return ' '.join(text.replace(\"'\", ' ').translate(str.maketrans('', '', string.punctuation)).split())\n",
    "\n",
    "\n",
    "def supp(text):\n",
    "    return text.replace(\"«\", \"\").replace(\"’\", \"\").replace(\"•\", \"\").replace(\"®\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import du vectorizer et du classifieur\n",
    "\n",
    "from joblib import load\n",
    "vectoriser_theme = load('vectorizer_classif_theme.joblib')\n",
    "classifier_theme = load('model_classif_theme.joblib')\n",
    "classifier_domaine = load('model_classif_domaine.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quest_user = \"ça va ?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Classification de la question user en domaine (to be CenterPark or not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quest_user_clean = supp(substitute_punctuation(stem_text(lemmatise_text(quest_user))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_quest_user = pd.Series(quest_user_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_quest_user_clean_vectorized_tfidf = vectoriser_theme.transform(X_quest_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "domaine_quest_user = classifier_domaine.predict(X_quest_user_clean_vectorized_tfidf)\n",
    "domaine_quest_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  A. Si dans le domaine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Classification de la question user en themes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redimension car NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX_quest_user = X_quest_user_clean_vectorized_tfidf.toarray().reshape(X_quest_user_clean_vectorized_tfidf.shape[0],1,\n",
    "                                                X_quest_user_clean_vectorized_tfidf.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prédiction avec le classifieur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba = classifier_theme.predict(XX_quest_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformation proba en classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "idx = np.argmax(pred_proba, axis=-1)\n",
    "YY_pred = np.zeros( pred_proba.shape )\n",
    "YY_pred[ np.arange(YY_pred.shape[0]), idx] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theme_quest_user = list(YY_pred[0]).index(1) +1\n",
    "theme_quest_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Trouver la question de la FAQ la plus proche/similaire de la question user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faq_theme = faq.iloc[theme_quest_user-1][[\"question\", 'reponse']]\n",
    "quest_user_clean = supp_sw(supp(substitute_punctuation(stem_text(lemmatise_text(quest_user)))))\n",
    "quest_user_clean_tokens = nlp(quest_user_clean)\n",
    "lst_similarity = [quest_user_clean_tokens.similarity(token) for token in faq_theme.tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Renvoyer la plus réponse de cette question FAQ (affichage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_quest_user = faq_theme.reponse[np.asarray(lst_similarity).argmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Pas dans le domaine : générer réponse originale avec le GAN sur text (partie Mathilde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tf.keras.models import model_from_json\n",
    "\n",
    "with open('model_generation_text.json', 'r') as json_file :\n",
    "    loaded_model_json = json_file.read()\n",
    "\n",
    "model_generation_text = tf.keras.models.model_from_json(loaded_model_json)\n",
    "model_generation_text.load_weights(\"model_generation_text.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints\\\\ckpt_10'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024\n",
    "model_generation_text = build_model(205, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model_generation_text.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model_generation_text.build(tf.TensorShape([1, None]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (1, None, 256)            52480     \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (1, None, 205)            210125    \n",
      "=================================================================\n",
      "Total params: 4,200,909\n",
      "Trainable params: 4,200,909\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement de char2idx et de idx2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('char2idx.pkl','rb') as f:\n",
    "    char2idx = pickle.load(f)\n",
    "    \n",
    "with open('idx2char.pkl','rb') as f:\n",
    "    idx2char = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction de génération d'une réponse originale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_eval = [char2idx[s] for s in u\"bonjour\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_eval = tf.expand_dims(input_eval, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=25882, shape=(1, 7), dtype=int32, numpy=array([[62, 75, 74, 70, 75, 81, 78]])>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=26199, shape=(1, 7, 205), dtype=float32, numpy=\n",
       "array([[[-3.0822709 , -2.3330765 , -2.9225059 , ..., -5.340893  ,\n",
       "         -0.7220677 , -0.50921637],\n",
       "        [-1.491913  ,  0.5688661 , -2.6871707 , ..., -5.9023824 ,\n",
       "         -2.9310274 ,  0.08995789],\n",
       "        [ 4.1873674 ,  2.7200687 , -0.6490847 , ..., -5.588496  ,\n",
       "         -6.2614093 , -4.497478  ],\n",
       "        ...,\n",
       "        [ 0.6209867 , -1.6301281 , -3.595356  , ..., -5.93286   ,\n",
       "         -4.853422  , -4.106124  ],\n",
       "        [ 2.587265  , -1.6352892 , -2.5301914 , ..., -7.614065  ,\n",
       "         -5.531     , -4.691694  ],\n",
       "        [ 8.382863  ,  2.1877823 ,  2.0526242 , ..., -9.441587  ,\n",
       "         -6.082235  , -8.444084  ]]], dtype=float32)>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_generation_text(input_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model_generation_text, start_string, num_generate=100): \n",
    "  # Evaluation step (generating text using the learned model)\n",
    "  # num_generate = Number of characters to generate\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 1.0\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model_generation_text.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model_generation_text(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the word returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Génération et affichage de la réponse originale selon la chaine de texte entrée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest(l):\n",
    "    maxi = 0\n",
    "    phrase = ''\n",
    "    for p in l:\n",
    "        if len(p)>maxi:\n",
    "            phrase = p\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Je t'ai vue main\n"
     ]
    }
   ],
   "source": [
    "print(longest(generate_text(model_generation_text, start_string=u\"Comment est votre blanquette\").split('.')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "quest_user = u\"Enora Claire Mathilde\"\n",
    "gene0 = generate_text(model_generation_text, start_string=quest_user)[len(quest_user):]\n",
    "gene = [p for p in gene0.split('.') if p!='' and len(p)>10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Simpsement de Bennetta'"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'unidecode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-239-a3a41c1d9718>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0munidecode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'unidecode'"
     ]
    }
   ],
   "source": [
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unichr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-241-148628ee4e8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0munichr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bonjuor\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'unichr' is not defined"
     ]
    }
   ],
   "source": [
    "unichr(\"bonjuor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
