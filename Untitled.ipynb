{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification de la question**\n",
    "\n",
    "Lorsque l'agent reçoit une question, il devra décider si la question est réellement liée au domaine métier ou non. Si oui et si les données sont en plus regroupées par thématiques, une deuxième décision est à prendre : sur laquelle de ces thématiques porte la question.\n",
    "\n",
    "Si c'est une question métier, le chatbot retournera une réponse pertinente selon sa stratégie ; si non, il déclenchera la composante conversationnelle, qui produira une réponse originale.\n",
    "\n",
    "Il faut donc mettre en place une stratégie pour la prise de ces décisions et pour la sélection de la réponse.\n",
    "\n",
    "Quelle que soit l'approche il faudra d'abord :\n",
    "\n",
    "    prétraiter la base de données (à faire une seule fois et à stocker). Attention, si on vectorise le corpus il faudra garder le vectoriseur (l'enregistrer comme pickle) pour appliquer ensuite le même vectoriseur à la question ;\n",
    "    prétraiter la question (en temps réel).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prétraitement textuel de la base de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lecture du fichier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>reponse</th>\n",
       "      <th>theme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Quels sont les jours d’arrivée ?</td>\n",
       "      <td>Il est possible d’arriver n’importe quel jour ...</td>\n",
       "      <td>Préparer mon séjour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Comment évaluer le confort de mon domaine et d...</td>\n",
       "      <td>Le classement par « birdies » évalue l’offre C...</td>\n",
       "      <td>Préparer mon séjour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Quels sont les services et activités compris d...</td>\n",
       "      <td>En réservant votre hébergement, vous bénéficie...</td>\n",
       "      <td>Préparer mon séjour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Comment réserver mes activités ?</td>\n",
       "      <td>Lors de la réservation de votre hébergement, v...</td>\n",
       "      <td>Préparer mon séjour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Où trouver le plan du domaine ?</td>\n",
       "      <td>Sur la page d'accueil de notre site, cliquez s...</td>\n",
       "      <td>Préparer mon séjour</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0                   Quels sont les jours d’arrivée ?   \n",
       "1  Comment évaluer le confort de mon domaine et d...   \n",
       "2  Quels sont les services et activités compris d...   \n",
       "3                   Comment réserver mes activités ?   \n",
       "4                    Où trouver le plan du domaine ?   \n",
       "\n",
       "                                             reponse                theme  \n",
       "0  Il est possible d’arriver n’importe quel jour ...  Préparer mon séjour  \n",
       "1  Le classement par « birdies » évalue l’offre C...  Préparer mon séjour  \n",
       "2  En réservant votre hébergement, vous bénéficie...  Préparer mon séjour  \n",
       "3  Lors de la réservation de votre hébergement, v...  Préparer mon séjour  \n",
       "4  Sur la page d'accueil de notre site, cliquez s...  Préparer mon séjour  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('faq_centerPark.pkl')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre jeu de données est composé de :\n",
    "- 54 lignes\n",
    "- 3 colonnes\n",
    "- 5 thèmes différents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {1 : \"Préparer mon séjour\",\n",
    "      2 : \"Réserver et payer\",\n",
    "      3 : \"Gérer ma réservation\",\n",
    "      4 : \"Mon séjour\",\n",
    "      5 : \"Assurances\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"theme\"].replace(\"Préparer mon séjour\", 1, inplace=True)\n",
    "df[\"theme\"].replace(\"Réserver et payer\", 2, inplace=True)\n",
    "df[\"theme\"].replace(\"Gérer ma réservation\", 3, inplace=True)\n",
    "df[\"theme\"].replace(\"Mon séjour\", 4, inplace=True)\n",
    "df[\"theme\"].replace(\"Assurances\", 5, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On pose :\n",
    "- 1 = Préparer mon séjour\n",
    "- 2 = Réserver et payer\n",
    "- 3 = Gérer ma réservation\n",
    "- 4 = Mon séjour\n",
    "- 5 = Assurances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enora\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4025: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  return super(DataFrame, self).rename(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "df1 = df[['question', 'theme']]\n",
    "df1.rename(columns={'question': 'texte', 'theme': 'theme'}, inplace=True)\n",
    "df2 = df[['reponse', 'theme']]\n",
    "df2.rename(columns={'reponse': 'texte', 'theme': 'theme'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df1, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texte</th>\n",
       "      <th>theme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Quels sont les jours d’arrivée ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Comment évaluer le confort de mon domaine et d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Quels sont les services et activités compris d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Comment réserver mes activités ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Où trouver le plan du domaine ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Puis-je venir avec mon animal domestique ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Peut-on accéder au domaine à la journée (sans ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Comment réserver un logement adapté aux person...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Comment recevoir la brochure ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Est-il possible d'acheter des billets pour les...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Un visa ou un passeport est-il impératif pour ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Comment utiliser l’offre Early Booking avec le...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Y a-t-il un service bagagerie pour déposer mes...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Y'a-t-il des frais de dossier ?</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Dois-je payer la totalité de mon séjour au mom...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Ai-je un droit de rétractation ?</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Comment payer si je réserve mon séjour sur le ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Comment payer si je réserve mon séjour via le ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Puis-je adresser mon règlement par courrier ?</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Comment puis-je payer si je ne réside pas en F...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                texte  theme\n",
       "0                    Quels sont les jours d’arrivée ?      1\n",
       "1   Comment évaluer le confort de mon domaine et d...      1\n",
       "2   Quels sont les services et activités compris d...      1\n",
       "3                    Comment réserver mes activités ?      1\n",
       "4                     Où trouver le plan du domaine ?      1\n",
       "5          Puis-je venir avec mon animal domestique ?      1\n",
       "6   Peut-on accéder au domaine à la journée (sans ...      1\n",
       "7   Comment réserver un logement adapté aux person...      1\n",
       "8                      Comment recevoir la brochure ?      1\n",
       "9   Est-il possible d'acheter des billets pour les...      1\n",
       "10  Un visa ou un passeport est-il impératif pour ...      1\n",
       "11  Comment utiliser l’offre Early Booking avec le...      1\n",
       "12  Y a-t-il un service bagagerie pour déposer mes...      1\n",
       "13                    Y'a-t-il des frais de dossier ?      2\n",
       "14  Dois-je payer la totalité de mon séjour au mom...      2\n",
       "15                   Ai-je un droit de rétractation ?      2\n",
       "16  Comment payer si je réserve mon séjour sur le ...      2\n",
       "17  Comment payer si je réserve mon séjour via le ...      2\n",
       "18      Puis-je adresser mon règlement par courrier ?      2\n",
       "19  Comment puis-je payer si je ne réside pas en F...      2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde\n",
    "df_concat.to_pickle('df_concat.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Définition de fonctions pour le nettoyage du texte des tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# pour le nettoyage du texte\n",
    "import nltk\n",
    "import string\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# pour la classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = pd.read_pickle('df_concat.pkl')\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_concat['texte'],\n",
    "                                                    df_concat['theme'],\n",
    "                                                    train_size=0.65,\n",
    "                                                    random_state=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('french')\n",
    "tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True) # tokenizer for tweet\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "\n",
    "sw = nltk.corpus.stopwords.words('french')\n",
    "sw += ['être', 'avoir']\n",
    "sw.sort()\n",
    "\n",
    "def lemmatise_text(text):\n",
    "    lst_lematised = [token.lemma_ for token in nlp(text)]\n",
    "    \n",
    "    return ' '.join(lst_lematised).lower()\n",
    "\n",
    "def stem_text(text):\n",
    "    lst_stemmerised = [stemmer.stem(token) for token in tokenizer.tokenize(text)]    \n",
    "    return ' '.join(lst_stemmerised)\n",
    "\n",
    "\n",
    "def replace_words_with_pos_tag(text):\n",
    "    lst_tags = [token.pos_ for token in nlp(text)]    \n",
    "    return ' '.join(lst_tags)\n",
    "\n",
    "def ner(text): #entites nommees\n",
    "    dico_remplacement = {entite_nommee.text : entite_nommee.label_ for entite_nommee in nlp(text).ents}\n",
    "    for entite_nommee, remplacement in dico_remplacement.items():\n",
    "        text = text.replace(entite_nommee, remplacement)\n",
    "    return text\n",
    "\n",
    "def substitute_punctuation(text):\n",
    "    return ' '.join(text.replace(\"'\", ' ').translate(str.maketrans('', '', string.punctuation)).split())\n",
    "\n",
    "def substitute_number(text, url_replacement=''):\n",
    "    return re.sub(r\"\\d\", url_replacement, text)\n",
    "\n",
    "def supp(text):\n",
    "    return text.replace(\"«\", \"\").replace(\"’\", \"\").replace(\"•\", \"\").replace(\"®\", \"\")\n",
    "\n",
    "def supprime_accent(txt):\n",
    "        accent = ['é', 'è', 'ê', 'à', 'ù', 'û', 'ç', 'ô', 'î', 'ï', 'â'] #liste des caractères accentués\n",
    "        sans_accent = ['e', 'e', 'e', 'a', 'u', 'u', 'c', 'o', 'i', 'i', 'a'] #liste des caractères non accentués\n",
    "        for i in range(0,len(accent)):\n",
    "            txt = txt.replace(accent[i], sans_accent[i]) #remplace les caractères accentués par l'équivalence sans accent (et en minuscule)\n",
    "        return txt #retourne l'entrée de l'utilisateur sans accents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Nettoyage du texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Nettoyage "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On combine quelques fonctions définies en partie A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_clean = (X_train.apply(lemmatise_text)\n",
    "                        .apply(stem_text)\n",
    "                        .apply(substitute_punctuation)\n",
    "                        .apply(supprime_accent)\n",
    "                        .apply(substitute_number)\n",
    "                        .apply(supp)\n",
    "                )\n",
    "X_test_clean = (X_test.apply(lemmatise_text)\n",
    "                       .apply(stem_text)\n",
    "                       .apply(substitute_punctuation)\n",
    "                       .apply(supprime_accent)\n",
    "                       .apply(substitute_number)\n",
    "                       .apply(supp)\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = \"comment «utilis l ’• offre early booking avec le\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comment utilis l  offre early booking avec le'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.replace(\"’\", \"\").replace(\"«\", \"\").replace(\"•\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = r\"[\\w^]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comment utilis l ’ offre early booking avec le'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.sub(\"(?<=[a-z])'(?=[a-z])\", \"\", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"^[']\", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51    1 vous pouv • nous contact au numero indiqu ic...\n",
       "13    oui le frais de dossi etre annuel et d ’ un mo...\n",
       "28    vous pouvoir utilis vos chequ vacanc ancv tick...\n",
       "16    en reserv sur internet vous pouvoir pai votr s...\n",
       "26                 comment puis je reserv pour un group\n",
       "52                      je n avoir pas souscrir d assur\n",
       "6     il etre possibl de pass le journ a cent parc s...\n",
       "52    notr assur pack protect total souscrit aupres ...\n",
       "10    pour le ressort de l ’ union europeen un vis o...\n",
       "21                 comment regl mon reserv avec un ancv\n",
       "25    puis je fair un partenariat commercial avec ce...\n",
       "33            pouvoir on etre plus nombreux que prevoir\n",
       "29    comment me assur que mon reserv etre bien enre...\n",
       "38    le rendu un cle se effectu a 10h le jour de vo...\n",
       "14    dois je pai le total de mon sejour au moment d...\n",
       "17    pour tout reserv au centr d ’ appel jusqu ’ au...\n",
       "3     lor de le reserv de votr heberg vous pouvoir d...\n",
       "1     le class par « bird » evalu l ’ offrir cent pa...\n",
       "25    vous pouvoir visit ce pag pour plus de inform ...\n",
       "3                              comment reserv mon activ\n",
       "43                          quel etre le regl a respect\n",
       "11    comment utilis l ’ offre early booking avec le...\n",
       "30    nous attir votr attent sur le fait que tout mo...\n",
       "45         dan le parc pouvoir on pai en ticket restaur\n",
       "35    1 vous pouv • nous contact au numero indiqu ic...\n",
       "50    notr assur voyag cent parc avoir etre cre sur ...\n",
       "41    wi fi basic gratuit disponibl dan tout le cott...\n",
       "29    un numero de reserv vous etre attribu et il co...\n",
       "19      comment puis je pai si je ne resid pas en franc\n",
       "38                     a quel heur se fair le check out\n",
       "                            ...                        \n",
       "49    oui uniqu sur le domain haut de bruyer situ a ...\n",
       "1     comment evalu le confort de mon domain et de m...\n",
       "53    declar votr sinistr dan le 5 jour qui suivent ...\n",
       "41                             le wi fi estil disponibl\n",
       "9     etre il possibl de achet un billet pour le par...\n",
       "18                   puis je adress mon regl par courri\n",
       "43    afin de facilit le deroul de votr sejour un re...\n",
       "47    quel etre le equip disponibl dan le cottag mac...\n",
       "11    profit de notr activ cadeal offrir pour tout r...\n",
       "21    profit de votr ancv pour vous offrir un sejour...\n",
       "23    tout reserv par bon ou chequ cadeau a moin de ...\n",
       "44           dan le parc pouvoir on pai en chequ vacanc\n",
       "44    le chequ vacanc ancv agenc national chequ vaca...\n",
       "42    il etre possibl de reserv le activ et servic l...\n",
       "53    comment percevoir mon indemn aupres d ’ europ ...\n",
       "15                            ai je un droit de retract\n",
       "22    pour tout pai par un moyen autr que le cart ba...\n",
       "7     comment reserv un log adapt aux person a mobil...\n",
       "26    vous pouvoir visit ce pag pour plus de inform ...\n",
       "30    je souhait modifi le lieu ou le dat de mon sejour\n",
       "27    puis je reserv mon activ a le recept et pai pa...\n",
       "8     cliqu ici pour rempl un demand de envoi de bro...\n",
       "46    notr cottag dispos de cuisin ouvert sur le sej...\n",
       "8                           comment recevoir le brochur\n",
       "19    le pai un person vivr hor de franc s ’ effectu...\n",
       "16    comment pai si je reserv mon sejour sur le sit...\n",
       "48    afin de profit de notr tarif avantag jusqu ’ a...\n",
       "7     tout notr domain franc dont villag natur ® par...\n",
       "24    notr prix s ’ entendr tout tax compr et inclue...\n",
       "45    le ticket restaur chequ dejeun ticket restaur ...\n",
       "Name: texte, Length: 70, dtype: object"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Test de différents modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons tester différents modèles sur le texte des tweets qui ont été nettoyés dans la partie 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Les différents vectoriseurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectoriseur numérique discret\n",
    "\n",
    "vect_count = CountVectorizer(binary=False)\n",
    "vect_count.fit(X_train_clean)\n",
    "X_train_clean_vectorized_count = vect_count.transform(X_train_clean)\n",
    "X_test_clean_vectorized_count = vect_count.transform(X_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectoriseur binaire\n",
    "\n",
    "bin_count = CountVectorizer(binary=True)\n",
    "bin_count.fit(X_train_clean)\n",
    "X_train_clean_vectorized_bin = vect_count.transform(X_train_clean)\n",
    "X_test_clean_vectorized_bin = vect_count.transform(X_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#vectoriseur numérique continu : TF-IDF\n",
    "\n",
    "vect_tfidf = TfidfVectorizer(stop_words=sw)\n",
    "vect_tfidf.fit(X_train_clean)\n",
    "X_train_clean_vectorized_tfidf = vect_tfidf.transform(X_train_clean)\n",
    "X_test_clean_vectorized_tfidf = vect_tfidf.transform(X_test_clean) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Les différents modèles\n",
    "\n",
    "Nous entraînerons des modèles de classification appartenant à quelques familles d'algorithmes d'apprentissage automatique classique. L'objectif est de comparer non seulement les performances des différentes méthodes entre elles, mais aussi la performance d'une même méthode sur des représentations différentes du texte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b.1 DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15789473684210525"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_uniform = DummyClassifier(strategy='uniform').fit(X_train_clean_vectorized_count, y_train)\n",
    "predictions_valid = random_uniform.predict(X_test_clean_vectorized_count)\n",
    "accuracy_score(y_test, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2894736842105263"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_uniform = DummyClassifier(strategy='most_frequent').fit(X_train_clean_vectorized_bin, y_train)\n",
    "predictions_valid = random_uniform.predict(X_test_clean_vectorized_bin)\n",
    "accuracy_score(y_test, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2894736842105263"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_prop_class = DummyClassifier(strategy='most_frequent').fit(X_train_clean_vectorized_tfidf, y_train)\n",
    "pred = random_prop_class.predict(X_test_clean_vectorized_tfidf)\n",
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b.2 Classifieur naïf bayesien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5789473684210527"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nb = MultinomialNB().fit(X_train_clean_vectorized_count, y_train)\n",
    "predictions_valid = model_nb.predict(X_test_clean_vectorized_count)\n",
    "accuracy_score(y_test, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5789473684210527"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nb = MultinomialNB().fit(X_train_clean_vectorized_bin, y_train)\n",
    "predictions_valid = model_nb.predict(X_test_clean_vectorized_bin)\n",
    "accuracy_score(y_test, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5789473684210527"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nb = MultinomialNB().fit(X_train_clean_vectorized_tfidf, y_train)\n",
    "predictions_valid = model_nb.predict(X_test_clean_vectorized_tfidf)\n",
    "accuracy_score(y_test, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_vectorized_bin_norm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-2ec7d0549abc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_nb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_vectorized_bin_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpredictions_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_nb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_vectorized_bin_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_vectorized_bin_norm' is not defined"
     ]
    }
   ],
   "source": [
    "model_nb = MultinomialNB().fit(X_train_vectorized_bin_norm, y_train_norm)\n",
    "predictions_valid = model_nb.predict(X_test_vectorized_bin_norm)\n",
    "accuracy_score(y_test_norm, predictions_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.3 Complement NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import ComplementNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.631578947368421"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_comp = ComplementNB().fit(X_train_clean_vectorized_count, y_train)\n",
    "predictions_valid = model_comp.predict(X_test_clean_vectorized_count)\n",
    "accuracy_score(y_test, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.631578947368421"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_comp = ComplementNB().fit(X_train_clean_vectorized_bin, y_train)\n",
    "predictions_valid = model_comp.predict(X_test_clean_vectorized_bin)\n",
    "accuracy_score(y_test, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6578947368421053"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_comp = ComplementNB().fit(X_train_clean_vectorized_tfidf, y_train)\n",
    "predictions_valid = model_comp.predict(X_test_clean_vectorized_tfidf)\n",
    "accuracy_score(y_test, predictions_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.4 BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4473684210526316"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bern = BernoulliNB().fit(X_train_clean_vectorized_count, y_train)\n",
    "predictions_valid = model_bern.predict(X_test_clean_vectorized_count)\n",
    "accuracy_score(y_test, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4473684210526316"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bern = BernoulliNB().fit(X_train_clean_vectorized_bin, y_train)\n",
    "predictions_valid = model_bern.predict(X_test_clean_vectorized_bin)\n",
    "accuracy_score(y_test, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42105263157894735"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bern = BernoulliNB().fit(X_train_clean_vectorized_tfidf, y_train)\n",
    "predictions_valid = model_bern.predict(X_test_clean_vectorized_tfidf)\n",
    "accuracy_score(y_test, predictions_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.5 CategoricalNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'CategoricalNB' from 'sklearn.naive_bayes' (C:\\Users\\enora\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-617625306788>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCategoricalNB\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'CategoricalNB' from 'sklearn.naive_bayes' (C:\\Users\\enora\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py)"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.6 KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15789473684210525"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_knn = KNeighborsClassifier(4).fit(X_train_clean_vectorized_count, y_train)\n",
    "predictions_valid = model_knn.predict(X_test_clean_vectorized_count)\n",
    "accuracy_score(y_test, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15789473684210525"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_knn = KNeighborsClassifier(4).fit(X_train_clean_vectorized_bin, y_train)\n",
    "predictions_valid = model_knn.predict(X_test_clean_vectorized_bin)\n",
    "accuracy_score(y_test, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5789473684210527"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_knn = KNeighborsClassifier(4).fit(X_train_clean_vectorized_tfidf, y_train)\n",
    "predictions_valid = model_knn.predict(X_test_clean_vectorized_tfidf)\n",
    "accuracy_score(y_test, predictions_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.7. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4473684210526316"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_svm = SVC(kernel='linear', C=0.1).fit(X_train_clean_vectorized_count, y_train)\n",
    "predictions_valid = model_svm.predict(X_test_clean_vectorized_count)\n",
    "accuracy_score(y_test, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4473684210526316"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_svm = SVC(kernel='linear', C=0.1).fit(X_train_clean_vectorized_bin, y_train)\n",
    "predictions_valid = model_svm.predict(X_test_clean_vectorized_bin)\n",
    "accuracy_score(y_test, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2894736842105263"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_svm = SVC(kernel='linear', C=0.1).fit(X_train_clean_vectorized_tfidf, y_train)\n",
    "predictions_valid = model_svm.predict(X_test_clean_vectorized_tfidf)\n",
    "accuracy_score(y_test, predictions_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle vectoriel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du vectoriseur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On passe au vectoriseur use_idf=False pour qu'il soit binaire (0 = absence du terme, 1 = présence du terme)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(lowercase=True, stop_words=None,\n",
    "                            ngram_range=(1, 1),\n",
    "                            use_idf=False, smooth_idf=True, # idf lissé\n",
    "                            sublinear_tf=False, norm='l2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création de la matrice termes-documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = vectorizer.fit_transform(df['question'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interrogation du corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous souhaitons trouver le document du corpus qui est le plus similaire à cette requête:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words('french')\n",
    "sw.append('les') # manque dans la liste, par exemple\n",
    "vect = vectorizer = TfidfVectorizer(lowercase=True, stop_words=None,\n",
    "                            ngram_range=(1, 1),\n",
    "                            use_idf=False, smooth_idf=True, # idf lissé\n",
    "                            sublinear_tf=False, norm='l2')\n",
    "dtm = vect.fit_transform(df)\n",
    "\n",
    "def vectorize_query(query_text):\n",
    "    query_file = 'query.txt'\n",
    "    with open(query_file, 'w', encoding=\"utf-8\") as out_f:\n",
    "        out_f.write(query_text)\n",
    "    query_vector = vect.transform([query_file])\n",
    "    os.unlink(query_file)\n",
    "    return query_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les salutations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction sera utilisé pour le message d'acceuil entré par l'utilisateur et la génération de la réponse correspondante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonction pour les salutations de départ\n",
    "\n",
    "salutations_inputs = (\"salut\", \"hey\", \"coucou\", \"bonjour\")\n",
    "salutations_responses = [\"bonjour et bienvenu.e\", \"bonjour\", \"bienvenu.e\"]\n",
    "\n",
    "def generate_greeting_response(greeting):\n",
    "    for token in greeting.split():\n",
    "        if token.lower() in greeting_inputs:\n",
    "            return random.choice(greeting_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons créer une fonction qui prend en entrée l'utilisateur, trouve la similitude en cosinus de \n",
    "l'entrée utilisateur et la compare avec les phrases du corpus.\n",
    "\n",
    "Source : https://stackabuse.com/python-for-nlp-creating-a-rule-based-chatbot/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "sw = stopwords.words('french')\n",
    "sw.append('les') # manque dans la liste, par exemple\n",
    "vect = vectorizer = TfidfVectorizer(lowercase=True, stop_words=sw,\n",
    "                            ngram_range=(1, 1),\n",
    "                            use_idf=False, smooth_idf=True, # idf lissé\n",
    "                            sublinear_tf=False, norm='l2')\n",
    "dtm = vect.fit_transform(df)\n",
    "\n",
    "#Vectorisation de l'input utilisateur\n",
    "def vectorize_query(query_text):\n",
    "    query_file = 'query.txt'\n",
    "    with open(query_file, 'w', encoding=\"utf-8\") as out_f:\n",
    "        out_f.write(query_text) #ecrire l'input utilisateur dans un fichier texte\n",
    "    query_vector = vect.transform([query_file])\n",
    "    os.unlink(query_file)\n",
    "    return query_vector\n",
    "\n",
    "query = [\"test.txt\"]\n",
    "query_vector = vect.transform(query)\n",
    "#query_vector = vectorize_query(query)\n",
    "query_corpus_sim = np.squeeze(cosine_similarity(dtm, query_vector))\n",
    "idx_most_sim = np.argmax(query_corpus_sim)\n",
    "df[idx_most_sim]\n",
    "print(df[idx_most_sim])\n",
    "\n",
    "def get_best_doc(query_text):\n",
    "    query_vector = vectorize_query(query_text)\n",
    "    query_corpus_sim = np.squeeze(cosine_similarity(dtm, query_vector))\n",
    "    doc_id = np.argmax(query_corpus_sim)\n",
    "    doc_path = df[doc_id] # contient le répertoire parent\n",
    "    return doc_path\n",
    "\n",
    "def print_result(query_text):\n",
    "    doc_path = get_best_doc(query_text)\n",
    "    doc_filename = os.path.split(doc_path)[-1] # sans le répertoire parent\n",
    "    print(doc_filename) # affiche le nom du fichier\n",
    "    print('-' * 20)     # affiche une ligne de '-'\n",
    "    with open(doc_path, 'r') as in_f:\n",
    "        print(in_f.read(500) + '...') # affiche les premiers 500 caractères du doc\n",
    "\n",
    "query = 'politique'\n",
    "print_result(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
